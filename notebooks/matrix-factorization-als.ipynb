{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization with Alternating Least Squares (ALS)\n",
    "\n",
    "This notebook implements ALS matrix factorization for collaborative filtering using the `implicit` library.\n",
    "\n",
    "## Overview\n",
    "- **Matrix Factorization**: Decompose user-item interaction matrix into user and item latent factors\n",
    "- **ALS Algorithm**: Alternating Least Squares optimization for implicit feedback\n",
    "- **Confidence Weighting**: Handle implicit feedback with confidence-based learning\n",
    "- **GPU Acceleration**: Leverage GPU for faster training when available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import pickle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a parquet DataFrame from disk.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the parquet DataFrame file.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with user-item interaction data.\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(path, engine=\"pyarrow\")\n",
    "\n",
    "\n",
    "def build_interaction_matrix(df: pd.DataFrame) -> Tuple[csr_matrix, pd.Index, pd.Index]:\n",
    "    \"\"\"\n",
    "    Build a sparse interaction matrix from a DataFrame of user-item interactions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns 'user_id' and 'click_article_id'.\n",
    "        \n",
    "    Returns:\n",
    "        mat: csr_matrix of shape (n_users, n_items), 1 indicates a click.\n",
    "        user_index: Index of unique user_id values.\n",
    "        item_index: Index of unique click_article_id values.\n",
    "    \"\"\"\n",
    "    users = df['user_id'].astype('category')\n",
    "    items = df['click_article_id'].astype('category')\n",
    "    user_codes = users.cat.codes.values\n",
    "    item_codes = items.cat.codes.values\n",
    "    n_users = users.cat.categories.size\n",
    "    n_items = items.cat.categories.size\n",
    "\n",
    "    mat = csr_matrix(\n",
    "        (np.ones(len(df), dtype=np.float32), (user_codes, item_codes)),\n",
    "        shape=(n_users, n_items)\n",
    "    )\n",
    "    return mat, users.cat.categories, items.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ALS Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_als(\n",
    "    train_sparse: csr_matrix,\n",
    "    factors: int = 50,\n",
    "    regularization: float = 0.01,\n",
    "    iterations: int = 15,\n",
    "    alpha: float = 40.0\n",
    ") -> AlternatingLeastSquares:\n",
    "    \"\"\"\n",
    "    Trains an ALS model on implicit feedback data.\n",
    "    \n",
    "    Args:\n",
    "        train_sparse: User-item interaction matrix (users x items)\n",
    "        factors: Number of latent factors\n",
    "        regularization: L2 regularization strength\n",
    "        iterations: Number of ALS iterations\n",
    "        alpha: Confidence weighting for implicit feedback\n",
    "        \n",
    "    Returns:\n",
    "        Trained ALS model\n",
    "    \"\"\"\n",
    "    print(f\"Training ALS with matrix shape {train_sparse.shape}\")\n",
    "    print(f\"Parameters: factors={factors}, reg={regularization}, iter={iterations}, alpha={alpha}\")\n",
    "    \n",
    "    # Create confidence matrix: C = 1 + alpha * R\n",
    "    confidence = train_sparse.copy()\n",
    "    confidence = confidence.multiply(alpha)\n",
    "    confidence.data += 1.0\n",
    "    \n",
    "    # Create ALS model\n",
    "    model = AlternatingLeastSquares(\n",
    "        factors=factors,\n",
    "        regularization=regularization,\n",
    "        iterations=iterations,\n",
    "        use_gpu=True,  # Use GPU if available\n",
    "        use_native=False  # Expect users x items matrix\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(confidence)\n",
    "    \n",
    "    print(f\"Model trained successfully!\")\n",
    "    print(f\"User factors shape: {model.user_factors.shape}\")\n",
    "    print(f\"Item factors shape: {model.item_factors.shape}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recommendation Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_als(\n",
    "    model: AlternatingLeastSquares,\n",
    "    train_sparse: csr_matrix,\n",
    "    user_index: pd.Index,\n",
    "    item_index: pd.Index,\n",
    "    top_k: int = 10\n",
    ") -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Generate top-k ALS recommendations for all users.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ALS model\n",
    "        train_sparse: Training interaction matrix\n",
    "        user_index: Mapping from user codes to original user IDs\n",
    "        item_index: Mapping from item codes to original item IDs\n",
    "        top_k: Number of recommendations per user\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping user_id to list of recommended item_ids\n",
    "    \"\"\"\n",
    "    recs = {}\n",
    "    n_users = min(train_sparse.shape[0], model.user_factors.shape[0])\n",
    "    \n",
    "    print(f\"Generating recommendations for {n_users} users...\")\n",
    "    \n",
    "    for u in range(n_users):\n",
    "        user_id = int(user_index[u])\n",
    "        \n",
    "        try:\n",
    "            # Get recommendations from model\n",
    "            recommended = model.recommend(\n",
    "                u, train_sparse[u], N=top_k, filter_already_liked_items=True\n",
    "            )\n",
    "            \n",
    "            # Extract item codes\n",
    "            if isinstance(recommended, tuple) and len(recommended) == 2:\n",
    "                item_codes = recommended[0]\n",
    "            else:\n",
    "                try:\n",
    "                    item_codes = [rec[0] for rec in recommended]\n",
    "                except (IndexError, TypeError):\n",
    "                    item_codes = recommended\n",
    "            \n",
    "            # Filter valid indices and convert to original item IDs\n",
    "            valid_item_codes = [icode for icode in item_codes if 0 <= icode < len(item_index)]\n",
    "            recs[user_id] = [int(item_index[icode]) for icode in valid_item_codes]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for user {u}: {e}\")\n",
    "            recs[user_id] = []  # Empty recommendations on error\n",
    "    \n",
    "    print(f\"Generated recommendations for {len(recs)} users\")\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(\n",
    "    recommendations: Dict[int, List[int]],\n",
    "    ground_truth_df: pd.DataFrame,\n",
    "    k_list: List[int] = [5, 10, 20]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate recommendations against ground truth.\n",
    "    \n",
    "    Args:\n",
    "        recommendations: Dictionary mapping user_ids to recommended item_ids.\n",
    "        ground_truth_df: DataFrame with held-out interactions.\n",
    "        k_list: List of k values for precision@k and recall@k.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Build held-out map: user_id â†’ set of held-out click_article_id\n",
    "    held_out_map: Dict[int, set] = {}\n",
    "    for _, row in ground_truth_df.iterrows():\n",
    "        u = int(row['user_id'])\n",
    "        i = int(row['click_article_id'])\n",
    "        held_out_map.setdefault(u, set()).add(i)\n",
    "\n",
    "    # Initialize metrics\n",
    "    sum_prec = {k: 0.0 for k in k_list}\n",
    "    sum_rec = {k: 0.0 for k in k_list}\n",
    "    sum_rank = 0.0\n",
    "    total_users = 0\n",
    "    total_held_items = 0\n",
    "\n",
    "    for u, rec_list in recommendations.items():\n",
    "        if u not in held_out_map:\n",
    "            continue\n",
    "        gt_items = held_out_map[u]\n",
    "        if not gt_items:\n",
    "            continue\n",
    "\n",
    "        total_users += 1\n",
    "        total_held_items += len(gt_items)\n",
    "\n",
    "        # Compute Precision@k and Recall@k\n",
    "        for k in k_list:\n",
    "            topk = set(rec_list[:k])\n",
    "            n_hit = len(topk & gt_items)\n",
    "            sum_prec[k] += n_hit / float(k)\n",
    "            sum_rec[k] += n_hit / float(len(gt_items))\n",
    "\n",
    "        # Compute Mean Rank\n",
    "        for item in gt_items:\n",
    "            if item in rec_list:\n",
    "                r = rec_list.index(item) + 1  # 1-based rank\n",
    "            else:\n",
    "                r = len(rec_list) + 1\n",
    "            sum_rank += r\n",
    "\n",
    "    # Aggregate results\n",
    "    results: Dict[str, float] = {}\n",
    "    for k in k_list:\n",
    "        results[f'precision@{k}'] = sum_prec[k] / total_users if total_users > 0 else 0.0\n",
    "        results[f'recall@{k}'] = sum_rec[k] / total_users if total_users > 0 else 0.0\n",
    "    results['mean_rank'] = sum_rank / total_held_items if total_held_items > 0 else float('inf')\n",
    "    results['total_users'] = total_users\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (use sample data for demo)\n",
    "try:\n",
    "    train_df = load_data('../data/sample/sample_interactions.csv')\n",
    "    valid_df = load_data('../data/sample/sample_interactions.csv')\n",
    "    test_df = load_data('../data/sample/sample_interactions.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Sample data not found. Please ensure sample datasets are available.\")\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic data\n",
    "    n_users, n_items, n_interactions = 1000, 500, 5000\n",
    "    \n",
    "    synthetic_data = pd.DataFrame({\n",
    "        'user_id': np.random.randint(0, n_users, n_interactions),\n",
    "        'click_article_id': np.random.randint(0, n_items, n_interactions),\n",
    "        'click_timestamp': pd.date_range('2024-01-01', periods=n_interactions, freq='1H')\n",
    "    })\n",
    "    \n",
    "    # Split into train/valid/test\n",
    "    train_df = synthetic_data.iloc[:3000].copy()\n",
    "    valid_df = synthetic_data.iloc[3000:4000].copy()\n",
    "    test_df = synthetic_data.iloc[4000:].copy()\n",
    "\n",
    "print(f\"Dataset shapes - Train: {train_df.shape}, Valid: {valid_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Interaction Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build interaction matrices\n",
    "print(\"Building interaction matrices...\")\n",
    "\n",
    "train_mat, user_index, item_index = build_interaction_matrix(train_df)\n",
    "valid_mat, _, _ = build_interaction_matrix(valid_df)\n",
    "test_mat, _, _ = build_interaction_matrix(test_df)\n",
    "\n",
    "print(f\"Training matrix shape: {train_mat.shape}\")\n",
    "print(f\"Training matrix density: {train_mat.nnz / (train_mat.shape[0] * train_mat.shape[1]):.6f}\")\n",
    "print(f\"Number of users: {len(user_index)}\")\n",
    "print(f\"Number of items: {len(item_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train ALS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ALS model with different configurations\n",
    "print(\"Training ALS model...\")\n",
    "\n",
    "# Hyperparameters\n",
    "als_model = train_als(\n",
    "    train_mat, \n",
    "    factors=64, \n",
    "    regularization=0.01, \n",
    "    iterations=20, \n",
    "    alpha=40.0\n",
    ")\n",
    "\n",
    "print(\"ALS training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "print(\"Generating recommendations...\")\n",
    "\n",
    "als_recs_valid = recommend_als(als_model, train_mat, user_index, item_index, top_k=10)\n",
    "als_recs_test = recommend_als(als_model, train_mat, user_index, item_index, top_k=10)\n",
    "\n",
    "print(f\"Generated recommendations for {len(als_recs_valid)} users\")\n",
    "\n",
    "# Show sample recommendations\n",
    "sample_user = next(iter(als_recs_valid.keys()))\n",
    "print(f\"\\nSample recommendations for user {sample_user}:\")\n",
    "print(als_recs_valid[sample_user][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "metrics_valid = evaluate_recommendations(als_recs_valid, valid_df, k_list=[5, 10, 20])\n",
    "\n",
    "print(\"\\nValidation Metrics (ALS):\")\n",
    "for metric, value in metrics_valid.items():\n",
    "    if metric != 'total_users':\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {int(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "metrics_test = evaluate_recommendations(als_recs_test, test_df, k_list=[5, 10, 20])\n",
    "\n",
    "print(\"\\nTest Metrics (ALS):\")\n",
    "for metric, value in metrics_test.items():\n",
    "    if metric != 'total_users':\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {int(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different hyperparameter settings\n",
    "print(\"\\nHyperparameter Analysis:\")\n",
    "print(\"Testing different factor sizes and alpha values...\")\n",
    "\n",
    "hyperparams = [\n",
    "    {'factors': 32, 'alpha': 20.0},\n",
    "    {'factors': 64, 'alpha': 40.0},\n",
    "    {'factors': 128, 'alpha': 40.0}\n",
    "]\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "for params in hyperparams:\n",
    "    print(f\"\\nTesting: factors={params['factors']}, alpha={params['alpha']}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = train_als(\n",
    "        train_mat,\n",
    "        factors=params['factors'],\n",
    "        regularization=0.01,\n",
    "        iterations=15,\n",
    "        alpha=params['alpha']\n",
    "    )\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recs = recommend_als(model, train_mat, user_index, item_index, top_k=10)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_recommendations(recs, valid_df, k_list=[5, 10])\n",
    "    \n",
    "    # Store results\n",
    "    result = params.copy()\n",
    "    result.update({\n",
    "        'precision@5': metrics['precision@5'],\n",
    "        'recall@10': metrics['recall@10'],\n",
    "        'mean_rank': metrics['mean_rank']\n",
    "    })\n",
    "    results_comparison.append(result)\n",
    "    \n",
    "    print(f\"  Precision@5: {metrics['precision@5']:.4f}\")\n",
    "    print(f\"  Recall@10: {metrics['recall@10']:.4f}\")\n",
    "    print(f\"  Mean Rank: {metrics['mean_rank']:.2f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nHyperparameter Comparison Summary:\")\n",
    "comparison_df = pd.DataFrame(results_comparison)\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect learned factors\n",
    "print(\"Model Factor Analysis:\")\n",
    "print(f\"User factors shape: {als_model.user_factors.shape}\")\n",
    "print(f\"Item factors shape: {als_model.item_factors.shape}\")\n",
    "\n",
    "# Analyze factor distributions\n",
    "user_factor_norms = np.linalg.norm(als_model.user_factors, axis=1)\n",
    "item_factor_norms = np.linalg.norm(als_model.item_factors, axis=1)\n",
    "\n",
    "print(f\"\\nUser factor norms - Mean: {user_factor_norms.mean():.3f}, Std: {user_factor_norms.std():.3f}\")\n",
    "print(f\"Item factor norms - Mean: {item_factor_norms.mean():.3f}, Std: {item_factor_norms.std():.3f}\")\n",
    "\n",
    "# Find most similar items to a sample item\n",
    "if len(item_index) > 0:\n",
    "    sample_item = 0\n",
    "    item_similarities = np.dot(als_model.item_factors, als_model.item_factors[sample_item])\n",
    "    most_similar = np.argsort(item_similarities)[-6:-1][::-1]  # Top 5 similar items\n",
    "    \n",
    "    print(f\"\\nMost similar items to item {item_index[sample_item]}:\")\n",
    "    for i, similar_idx in enumerate(most_similar):\n",
    "        similarity = item_similarities[similar_idx]\n",
    "        print(f\"  {i+1}. Item {item_index[similar_idx]} (similarity: {similarity:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusions\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **ALS Matrix Factorization**: Implemented using the `implicit` library for efficient training\n",
    "2. **Confidence Weighting**: Applied to handle implicit feedback appropriately\n",
    "3. **Hyperparameter Tuning**: Compared different factor sizes and alpha values\n",
    "4. **Comprehensive Evaluation**: Used multiple metrics to assess performance\n",
    "\n",
    "### Key Insights:\n",
    "- **Latent Factors**: ALS learns meaningful user and item representations\n",
    "- **Scalability**: Efficient for large-scale datasets with sparse interactions\n",
    "- **Hyperparameters**: Factor size and confidence weighting significantly impact performance\n",
    "- **Cold Start**: Performs poorly for users/items with no training interactions\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different regularization values\n",
    "- Implement item-based ALS for comparison\n",
    "- Combine ALS with other algorithms in an ensemble\n",
    "- Add content-based features to handle cold start users"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}