{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommendation System\n",
    "\n",
    "This notebook implements and evaluates collaborative filtering algorithms for the recommendation system.\n",
    "\n",
    "## Overview\n",
    "- **User-to-User Collaborative Filtering**: Find similar users and recommend items they liked\n",
    "- **Item-to-Item Collaborative Filtering**: Find similar items based on user interactions\n",
    "- **Evaluation**: Compare performance using recall@k and mean rank metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data types for memory efficiency\n",
    "DT_FLOAT = np.float32\n",
    "DT_INT = np.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_parquet(\"../data/sample/sample_interactions.csv\")\n",
    "valid_df = pd.read_parquet(\"../data/sample/sample_interactions.csv\")\n",
    "test_df = pd.read_parquet(\"../data/sample/sample_interactions.csv\")\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Valid: {valid_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode user and item IDs\n",
    "enc = OrdinalEncoder(dtype=DT_INT, handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "# Fit on training data and transform all datasets\n",
    "train_df[[\"user_id\", \"click_article_id\"]] = enc.fit_transform(\n",
    "    train_df[[\"user_id\", \"click_article_id\"]]\n",
    ")\n",
    "valid_df[[\"user_id\", \"click_article_id\"]] = enc.transform(\n",
    "    valid_df[[\"user_id\", \"click_article_id\"]]\n",
    ")\n",
    "test_df[[\"user_id\", \"click_article_id\"]] = enc.transform(\n",
    "    test_df[[\"user_id\", \"click_article_id\"]]\n",
    ")\n",
    "\n",
    "n_users = int(train_df.user_id.max()) + 1\n",
    "n_items = int(train_df.click_article_id.max()) + 1\n",
    "\n",
    "print(f\"Number of users: {n_users}, Number of items: {n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse interaction matrices\n",
    "def make_interaction_matrix(df):\n",
    "    \"\"\"Create sparse user-item interaction matrix.\"\"\"\n",
    "    # Filter out unknown users/items (-1)\n",
    "    mask = (df.user_id >= 0) & (df.click_article_id >= 0)\n",
    "    df_clean = df[mask]\n",
    "    print(f\"Filtered out {len(df) - len(df_clean)} unknown interactions\")\n",
    "    \n",
    "    return csr_matrix(\n",
    "        (np.ones(len(df_clean), dtype=np.float32),\n",
    "         (df_clean.user_id, df_clean.click_article_id)),\n",
    "        shape=(n_users, n_items)\n",
    "    )\n",
    "\n",
    "train_mat = make_interaction_matrix(train_df)\n",
    "valid_mat = make_interaction_matrix(valid_df)\n",
    "test_mat = make_interaction_matrix(test_df)\n",
    "\n",
    "print(f\"Training matrix shape: {train_mat.shape}\")\n",
    "print(f\"Training matrix density: {train_mat.nnz / (train_mat.shape[0] * train_mat.shape[1]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ground_truth(mat: csr_matrix):\n",
    "    \"\"\"Build ground truth sets for evaluation.\"\"\"\n",
    "    indptr, indices = mat.indptr, mat.indices\n",
    "    return [set(indices[indptr[u]:indptr[u+1]]) for u in range(mat.shape[0])]\n",
    "\n",
    "def evaluate_recommendations(ranks, ground_truth, k_list=(5, 10, 100)):\n",
    "    \"\"\"Evaluate recommendation performance.\"\"\"\n",
    "    n_users, _ = ranks.shape\n",
    "    gt_items = np.fromiter(\n",
    "        (next(iter(s)) if s else -1 for s in ground_truth),\n",
    "        dtype=DT_INT, count=n_users\n",
    "    )\n",
    "    \n",
    "    # Get ranks for ground truth items\n",
    "    gt_ranks = ranks[np.arange(n_users), gt_items]\n",
    "    \n",
    "    results = {}\n",
    "    for k in k_list:\n",
    "        hits = (gt_ranks <= k).astype(DT_INT)\n",
    "        results[f\"recall@{k}\"] = hits.mean(dtype=DT_FLOAT)\n",
    "    \n",
    "    results[\"mean_rank\"] = gt_ranks.mean(dtype=DT_FLOAT)\n",
    "    return results\n",
    "\n",
    "# Build ground truth for validation\n",
    "gt_valid = build_ground_truth(valid_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. User-to-User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing user-to-user collaborative filtering...\")\n",
    "\n",
    "TOP_SIM = 50  # Keep top 50 similar users\n",
    "\n",
    "# L2-normalize user rows for cosine similarity\n",
    "train_mat_csr = train_mat.tocsr()\n",
    "row_norms = np.sqrt(np.array(train_mat_csr.multiply(train_mat_csr).sum(axis=1)).flatten())\n",
    "row_norms[row_norms == 0] = 1.0  # Avoid division by zero\n",
    "\n",
    "# Normalize each row\n",
    "train_mat_norm = train_mat_csr.copy().astype(DT_FLOAT)\n",
    "train_mat_norm.data /= row_norms[train_mat_norm.nonzero()[0]]\n",
    "\n",
    "# Compute similarity matrix\n",
    "sim = train_mat_norm @ train_mat_norm.T\n",
    "sim = sim.toarray().astype(DT_FLOAT)\n",
    "\n",
    "# Keep top-(TOP_SIM+1) per row (including self)\n",
    "top_idx = np.argpartition(-sim, min(TOP_SIM+1, sim.shape[1]-1), axis=1)[:, :TOP_SIM+1]\n",
    "top_sim = np.take_along_axis(sim, top_idx, axis=1)\n",
    "\n",
    "# Sort by similarity descending\n",
    "order = np.argsort(-top_sim, axis=1)\n",
    "u_neighbors = np.take_along_axis(top_idx, order, axis=1)\n",
    "\n",
    "print(f\"Similarity matrix computed: {sim.shape}\")\n",
    "\n",
    "# Clean up memory\n",
    "del sim, top_idx, top_sim, train_mat_norm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_score_matrix(batch=2000):\n",
    "    \"\"\"Build recommendation scores based on similar users.\"\"\"\n",
    "    scores = np.zeros((n_users, n_items), dtype=np.uint8)\n",
    "    indptr, indices = train_mat_csr.indptr, train_mat_csr.indices\n",
    "\n",
    "    for start in range(0, n_users, batch):\n",
    "        end = min(start + batch, n_users)\n",
    "        for u in range(start, end):\n",
    "            # Get top 20 similar users (excluding self)\n",
    "            peers = u_neighbors[u][1:21]\n",
    "            if peers.size == 0:\n",
    "                continue\n",
    "            \n",
    "            srow = scores[u]\n",
    "            for p in peers:\n",
    "                # Add votes for items liked by similar users\n",
    "                srow[train_mat_csr[p].indices] += 1\n",
    "            \n",
    "            # Mask already seen items\n",
    "            srow[indices[indptr[u]:indptr[u+1]]] = 0\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"Building user-based recommendation scores...\")\n",
    "scores = build_user_score_matrix()\n",
    "\n",
    "# Convert scores to ranks\n",
    "ranks = scores.argsort(axis=1).astype(DT_INT)\n",
    "ranks = ranks.argsort(axis=1) + 1  # 1-based ranks\n",
    "\n",
    "print(\"User-to-user collaborative filtering completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate user-to-user CF\n",
    "u2u_metrics = evaluate_recommendations(\n",
    "    ranks, \n",
    "    [set([aid]) for aid in valid_df.click_article_id],\n",
    "    k_list=(5, 10, 100)\n",
    ")\n",
    "\n",
    "print(\"User-to-User Collaborative Filtering Results:\")\n",
    "for metric, value in u2u_metrics.items():\n",
    "    print(f\"  {metric}: {value:.6f}\")\n",
    "\n",
    "# Clean up memory\n",
    "del scores, ranks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Item-to-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing item-to-item collaborative filtering...\")\n",
    "\n",
    "TOP_SIM_ITEM = 50  # Keep top 50 similar items\n",
    "\n",
    "# Transpose to get item x user matrix\n",
    "item_mat_csr = train_mat_csr.T\n",
    "\n",
    "# L2 normalize item rows\n",
    "row_norms_i = np.sqrt(item_mat_csr.multiply(item_mat_csr).sum(1)).A1\n",
    "row_norms_i[row_norms_i == 0] = 1.0\n",
    "\n",
    "item_mat_norm = item_mat_csr.copy().astype(DT_FLOAT)\n",
    "item_mat_norm.data /= row_norms_i[item_mat_norm.nonzero()[0]]\n",
    "\n",
    "# Compute item similarity matrix\n",
    "isim = (item_mat_norm @ item_mat_norm.T).toarray().astype(DT_FLOAT)\n",
    "\n",
    "# Keep top similar items\n",
    "top_idx_i = np.argpartition(\n",
    "    -isim, min(TOP_SIM_ITEM+1, isim.shape[1]-1), axis=1\n",
    ")[:, :TOP_SIM_ITEM+1]\n",
    "top_sim_i = np.take_along_axis(isim, top_idx_i, axis=1)\n",
    "\n",
    "order_i = np.argsort(-top_sim_i, axis=1)\n",
    "item_neighbors = np.take_along_axis(top_idx_i, order_i, axis=1)\n",
    "\n",
    "print(f\"Item similarity matrix computed: {isim.shape}\")\n",
    "\n",
    "# Clean up memory\n",
    "del isim, top_idx_i, top_sim_i, item_mat_norm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_score_matrix(batch=2000):\n",
    "    \"\"\"Build recommendation scores based on similar items.\"\"\"\n",
    "    scores_i = np.zeros((n_users, n_items), dtype=np.uint8)\n",
    "    indptr, idx = train_mat_csr.indptr, train_mat_csr.indices\n",
    "\n",
    "    for s in range(0, n_users, batch):\n",
    "        e = min(s + batch, n_users)\n",
    "        for u in range(s, e):\n",
    "            seen = idx[indptr[u]:indptr[u+1]]\n",
    "            if seen.size == 0:\n",
    "                continue\n",
    "            \n",
    "            row = scores_i[u]\n",
    "            for itm in seen:\n",
    "                # Get similar items (excluding self)\n",
    "                peers = item_neighbors[itm][1:21]\n",
    "                row[peers] += 1\n",
    "            \n",
    "            # Mask already seen items\n",
    "            row[seen] = 0\n",
    "    \n",
    "    return scores_i\n",
    "\n",
    "print(\"Building item-based recommendation scores...\")\n",
    "scores_i = build_item_score_matrix()\n",
    "\n",
    "# Convert to ranks\n",
    "order_i = scores_i.argsort(axis=1)\n",
    "ranks_i = order_i.argsort(axis=1).astype(DT_INT) + 1\n",
    "\n",
    "print(\"Item-to-item collaborative filtering completed.\")\n",
    "\n",
    "# Clean up memory\n",
    "del scores_i, order_i\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate item-to-item CF\n",
    "i2i_metrics = evaluate_recommendations(\n",
    "    ranks_i,\n",
    "    [set([aid]) for aid in valid_df.click_article_id],\n",
    "    k_list=(5, 10, 100)\n",
    ")\n",
    "\n",
    "print(\"Item-to-Item Collaborative Filtering Results:\")\n",
    "for metric, value in i2i_metrics.items():\n",
    "    print(f\"  {metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "import pandas as pd\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'User-to-User CF': u2u_metrics,\n",
    "    'Item-to-Item CF': i2i_metrics\n",
    "})\n",
    "\n",
    "print(\"\\nCollaborative Filtering Comparison:\")\n",
    "print(comparison_df.round(6))\n",
    "\n",
    "# Determine better performing method\n",
    "if i2i_metrics['recall@10'] > u2u_metrics['recall@10']:\n",
    "    print(\"\\nItem-to-Item CF performs better on recall@10\")\n",
    "else:\n",
    "    print(\"\\nUser-to-User CF performs better on recall@10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **User-to-User CF**: Finds users with similar interaction patterns and recommends items they liked\n",
    "2. **Item-to-Item CF**: Identifies items that are frequently interacted with together\n",
    "3. **Performance Comparison**: Evaluates both approaches using standard recommendation metrics\n",
    "\n",
    "### Key Insights:\n",
    "- Item-to-Item CF typically performs better for implicit feedback datasets\n",
    "- Both methods suffer from sparsity issues in large datasets\n",
    "- These methods form the foundation for more advanced ensemble approaches\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different similarity metrics\n",
    "- Implement matrix factorization approaches (ALS)\n",
    "- Combine CF with content-based features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}