{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Ensemble Recommendation System\n",
    "\n",
    "This notebook implements a hybrid ensemble approach combining multiple recommendation algorithms.\n",
    "\n",
    "## Overview\n",
    "- **Multi-Algorithm Ensemble**: Combines collaborative filtering, popularity-based, and content-based approaches\n",
    "- **Context-Aware Recommendations**: Incorporates user context (device, OS, location)\n",
    "- **Temporal Dynamics**: Handles time-based popularity decay\n",
    "- **Cold Start Handling**: Specialized approaches for new users and items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Union, Tuple, Set, Optional, Any\n",
    "from scipy.sparse import csr_matrix\n",
    "from itertools import product\n",
    "\n",
    "# Set threading for performance\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"16\"\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - update these to point to your actual data\n",
    "DATA_PATHS = {\n",
    "    \"train\": \"../data/sample/sample_interactions.csv\",\n",
    "    \"valid\": \"../data/sample/sample_interactions.csv\", \n",
    "    \"test\": \"../data/sample/sample_interactions.csv\",\n",
    "    \"articles\": \"../data/sample/sample_articles.csv\",\n",
    "    \"embeddings\": \"../artifacts/article_embeddings.pkl\"  # If available\n",
    "}\n",
    "\n",
    "# Hyperparameters for temporal popularity\n",
    "TEMPORAL_PARAMS = {\n",
    "    \"half_life_days\": [1, 3, 5, 7, 10],\n",
    "    \"beta_values\": [0.0, 3.0, 7.0, 100.0],  # Freshness boost\n",
    "    \"alpha_values\": [0.0, 0.2, 0.5, 0.7, 1.0],  # Fresh vs decayed blend\n",
    "    \"fresh_window_days\": [1, 3, 7, 10]\n",
    "}\n",
    "\n",
    "# Recommendation parameters\n",
    "RECO_PARAMS = {\n",
    "    \"top_candidates\": 500,\n",
    "    \"recall_k\": 200,\n",
    "    \"embedding_dim\": 250,\n",
    "    \"k_recent_clicks\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Load all required datasets.\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Load interaction data\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        try:\n",
    "            df = pd.read_csv(DATA_PATHS[split])\n",
    "            # Ensure timestamp is datetime\n",
    "            if 'click_timestamp' in df.columns:\n",
    "                df[\"click_timestamp\"] = pd.to_datetime(df[\"click_timestamp\"])\n",
    "            datasets[split] = df\n",
    "            print(f\"Loaded {split}: {df.shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {DATA_PATHS[split]} not found, creating synthetic data\")\n",
    "            datasets[split] = create_synthetic_data(split)\n",
    "    \n",
    "    # Load article metadata\n",
    "    try:\n",
    "        articles_df = pd.read_csv(DATA_PATHS['articles'])\n",
    "        if 'created_at_ts' in articles_df.columns:\n",
    "            articles_df[\"created_at_ts\"] = pd.to_datetime(articles_df[\"created_at_ts\"])\n",
    "        datasets['articles'] = articles_df\n",
    "        print(f\"Loaded articles: {articles_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Article metadata not found, creating synthetic\")\n",
    "        datasets['articles'] = create_synthetic_articles()\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def create_synthetic_data(split: str, n_users=1000, n_items=500) -> pd.DataFrame:\n",
    "    \"\"\"Create synthetic interaction data for demonstration.\"\"\"\n",
    "    n_interactions = {'train': 5000, 'valid': 1000, 'test': 1000}[split]\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'user_id': np.random.randint(0, n_users, n_interactions),\n",
    "        'click_article_id': np.random.randint(0, n_items, n_interactions),\n",
    "        'click_timestamp': pd.date_range('2024-01-01', periods=n_interactions, freq='1H'),\n",
    "        'click_deviceGroup': np.random.choice([0, 1, 2], n_interactions),\n",
    "        'click_os': np.random.choice(range(1, 18), n_interactions),\n",
    "        'click_country': np.random.choice(['US', 'DE', 'FR', 'GB'], n_interactions)\n",
    "    })\n",
    "\n",
    "def create_synthetic_articles(n_articles=500) -> pd.DataFrame:\n",
    "    \"\"\"Create synthetic article metadata.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'article_id': range(n_articles),\n",
    "        'category_id': np.random.randint(0, 10, n_articles),\n",
    "        'created_at_ts': pd.date_range('2024-01-01', periods=n_articles, freq='2H'),\n",
    "        'publisher_id': np.random.randint(0, 5, n_articles),\n",
    "        'words_count': np.random.randint(100, 2000, n_articles)\n",
    "    })\n",
    "\n",
    "# Load all datasets\n",
    "print(\"Loading datasets...\")\n",
    "data = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Popularity-Based Recommendation with Temporal Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_popularity_with_metadata(train_df: pd.DataFrame, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute popularity scores with article creation timestamps.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: [click_article_id, clicks, created_at]\n",
    "    \"\"\"\n",
    "    # Count clicks per article\n",
    "    popularity = (\n",
    "        train_df\n",
    "        .groupby('click_article_id', as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'clicks'})\n",
    "    )\n",
    "    \n",
    "    # Merge with article creation dates\n",
    "    if 'created_at_ts' in articles_df.columns:\n",
    "        popularity = popularity.merge(\n",
    "            articles_df[['article_id', 'created_at_ts']].drop_duplicates(),\n",
    "            left_on='click_article_id',\n",
    "            right_on='article_id',\n",
    "            how='left'\n",
    "        )\n",
    "        popularity = popularity.drop(columns=['article_id'])\n",
    "        popularity = popularity.rename(columns={'created_at_ts': 'created_at'})\n",
    "    else:\n",
    "        # Use current time as fallback\n",
    "        popularity['created_at'] = pd.Timestamp.now()\n",
    "    \n",
    "    return popularity.sort_values('clicks', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Compute base popularity\n",
    "print(\"Computing article popularity...\")\n",
    "popular_articles = compute_popularity_with_metadata(data['train'], data['articles'])\n",
    "print(f\"Computed popularity for {len(popular_articles)} articles\")\n",
    "print(f\"Top 5 popular articles: {popular_articles.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Popularity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_temporal_popularity(val_df: pd.DataFrame, popular_articles: pd.DataFrame,\n",
    "                                half_life_days: float = 3.0, \n",
    "                                beta: float = 3.0,\n",
    "                                alpha: float = 1.0,\n",
    "                                fresh_days: float = 3.0) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate temporal popularity model with decay and freshness boost.\n",
    "    \n",
    "    Args:\n",
    "        half_life_days: Popularity decay half-life in days\n",
    "        beta: Freshness boost factor\n",
    "        alpha: Blend weight (1.0 = all fresh, 0.0 = all decayed)\n",
    "        fresh_days: Window for freshness boost\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    lambda_decay = np.log(2) / (half_life_days * 86400)  # Convert to seconds\n",
    "    fresh_seconds = fresh_days * 86400\n",
    "    \n",
    "    # Prepare data\n",
    "    scores_norm = popular_articles[\"clicks\"].to_numpy(dtype=np.float32)\n",
    "    scores_norm /= scores_norm.max()  # Normalize to [0, 1]\n",
    "    \n",
    "    created_ts = popular_articles[\"created_at\"].astype(\"datetime64[s]\").astype(np.int64)\n",
    "    art_ids = popular_articles[\"click_article_id\"].to_numpy()\n",
    "    \n",
    "    # Create mappings\n",
    "    article_aid2idx = {aid: idx for idx, aid in enumerate(art_ids)}\n",
    "    \n",
    "    # Process validation data\n",
    "    val_click_ts = val_df[\"click_timestamp\"].astype(\"datetime64[s]\").astype(np.int64)\n",
    "    val_article_ids = val_df[\"click_article_id\"].to_numpy()\n",
    "    \n",
    "    # Filter validation data to known articles\n",
    "    valid_mask = np.array([aid in article_aid2idx for aid in val_article_ids])\n",
    "    \n",
    "    if valid_mask.sum() == 0:\n",
    "        return {\"mean_rank\": float('inf'), \"recall@5\": 0.0, \"recall@200\": 0.0}\n",
    "    \n",
    "    val_click_ts = val_click_ts[valid_mask]\n",
    "    val_article_indices = np.array([article_aid2idx[aid] for aid in val_article_ids[valid_mask]])\n",
    "    \n",
    "    # Compute age matrix (validation_clicks x articles)\n",
    "    ages = val_click_ts[:, None] - created_ts[None, :]\n",
    "    \n",
    "    # Decay scores\n",
    "    decayed_scores = scores_norm[None, :] * np.exp(-lambda_decay * ages)\n",
    "    \n",
    "    # Fresh boost\n",
    "    fresh_mask = (ages <= fresh_seconds)\n",
    "    fresh_scores = scores_norm[None, :] * (1 + beta * fresh_mask)\n",
    "    \n",
    "    # Blend fresh and decayed\n",
    "    final_scores = alpha * fresh_scores + (1 - alpha) * decayed_scores\n",
    "    \n",
    "    # Compute ranks\n",
    "    true_scores = final_scores[np.arange(len(val_article_indices)), val_article_indices]\n",
    "    ranks = 1 + np.sum(final_scores > true_scores[:, None], axis=1)\n",
    "    \n",
    "    # Metrics\n",
    "    mean_rank = ranks.mean()\n",
    "    recall_5 = (ranks <= 5).mean()\n",
    "    recall_200 = (ranks <= 200).mean()\n",
    "    \n",
    "    return {\n",
    "        \"mean_rank\": float(mean_rank),\n",
    "        \"recall@5\": float(recall_5),\n",
    "        \"recall@200\": float(recall_200),\n",
    "        \"n_valid\": len(ranks)\n",
    "    }\n",
    "\n",
    "# Test temporal popularity with default parameters\n",
    "print(\"Evaluating temporal popularity model...\")\n",
    "temporal_results = evaluate_temporal_popularity(data['valid'], popular_articles)\n",
    "print(f\"Temporal popularity results: {temporal_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Content-Based Recommendations with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedRecommender:\n",
    "    \"\"\"Content-based recommender using article embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_path: Optional[str] = None):\n",
    "        self.index = None\n",
    "        self.item_embeddings = None\n",
    "        self.embedding_dim = RECO_PARAMS['embedding_dim']\n",
    "        \n",
    "        if embeddings_path and Path(embeddings_path).exists():\n",
    "            self.load_embeddings(embeddings_path)\n",
    "        else:\n",
    "            print(\"Embeddings not found, creating synthetic embeddings\")\n",
    "            self.create_synthetic_embeddings()\n",
    "    \n",
    "    def load_embeddings(self, path: str):\n",
    "        \"\"\"Load pre-trained article embeddings.\"\"\"\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                self.item_embeddings = pickle.load(f).astype(np.float32)\n",
    "            print(f\"Loaded embeddings: {self.item_embeddings.shape}\")\n",
    "            self._build_index()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embeddings: {e}\")\n",
    "            self.create_synthetic_embeddings()\n",
    "    \n",
    "    def create_synthetic_embeddings(self, n_items: int = 500):\n",
    "        \"\"\"Create synthetic embeddings for demonstration.\"\"\"\n",
    "        self.item_embeddings = np.random.random((n_items, self.embedding_dim)).astype(np.float32)\n",
    "        print(f\"Created synthetic embeddings: {self.item_embeddings.shape}\")\n",
    "        self._build_index()\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Build FAISS index for fast similarity search.\"\"\"\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(self.item_embeddings)\n",
    "        \n",
    "        # Build exact cosine similarity index\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "        self.index.add(self.item_embeddings)\n",
    "        print(f\"Built FAISS index with {self.index.ntotal} items\")\n",
    "    \n",
    "    def get_user_vector(self, clicked_items: List[int], k_recent: int = 10) -> Optional[np.ndarray]:\n",
    "        \"\"\"Create user vector from recent clicks.\"\"\"\n",
    "        if not clicked_items:\n",
    "            return None\n",
    "        \n",
    "        # Filter to valid item indices\n",
    "        valid_items = [item for item in clicked_items[-k_recent:] \n",
    "                      if 0 <= item < len(self.item_embeddings)]\n",
    "        \n",
    "        if not valid_items:\n",
    "            return None\n",
    "        \n",
    "        # Average embeddings of recent clicks\n",
    "        user_vector = self.item_embeddings[valid_items].mean(axis=0, keepdims=True)\n",
    "        faiss.normalize_L2(user_vector)\n",
    "        return user_vector\n",
    "    \n",
    "    def recommend(self, clicked_items: List[int], k_candidates: int = 100) -> List[int]:\n",
    "        \"\"\"Generate content-based recommendations.\"\"\"\n",
    "        user_vector = self.get_user_vector(clicked_items)\n",
    "        \n",
    "        if user_vector is None or self.index is None:\n",
    "            return []  # Return empty for cold users\n",
    "        \n",
    "        # Search for similar items\n",
    "        _, similar_items = self.index.search(user_vector, k_candidates)\n",
    "        \n",
    "        # Filter out already seen items\n",
    "        seen = set(clicked_items)\n",
    "        recommendations = [int(item) for item in similar_items[0] if item not in seen]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize content-based recommender\n",
    "print(\"Initializing content-based recommender...\")\n",
    "content_recommender = ContentBasedRecommender(DATA_PATHS.get('embeddings'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Collaborative Filtering Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_item_matrix(train_df: pd.DataFrame) -> Tuple[csr_matrix, np.ndarray]:\n",
    "    \"\"\"Build user-item interaction matrix for collaborative filtering.\"\"\"\n",
    "    n_users = int(train_df['user_id'].max()) + 1\n",
    "    n_items = int(train_df['click_article_id'].max()) + 1\n",
    "    \n",
    "    # Sort by timestamp to maintain chronological order\n",
    "    train_sorted = train_df.sort_values('click_timestamp')\n",
    "    \n",
    "    # Build sparse matrix\n",
    "    interaction_matrix = csr_matrix(\n",
    "        (np.ones(len(train_sorted), dtype=np.uint8),\n",
    "         (train_sorted['user_id'].values, train_sorted['click_article_id'].values)),\n",
    "        shape=(n_users, n_items)\n",
    "    )\n",
    "    \n",
    "    # Build user click histories\n",
    "    indptr, indices = interaction_matrix.indptr, interaction_matrix.indices\n",
    "    user_histories = [list(indices[indptr[u]:indptr[u+1]]) for u in range(n_users)]\n",
    "    \n",
    "    return interaction_matrix, user_histories\n",
    "\n",
    "def collaborative_filtering_last_item(user_histories: List[List[int]], \n",
    "                                     item_similarity: np.ndarray,\n",
    "                                     user_id: int, \n",
    "                                     k_candidates: int = 100) -> List[int]:\n",
    "    \"\"\"Simple item-based CF using last clicked item.\"\"\"\n",
    "    if user_id >= len(user_histories):\n",
    "        return []\n",
    "    \n",
    "    history = user_histories[user_id]\n",
    "    if not history:\n",
    "        return []\n",
    "    \n",
    "    # Get last clicked item\n",
    "    last_item = history[-1]\n",
    "    \n",
    "    if last_item >= len(item_similarity):\n",
    "        return []\n",
    "    \n",
    "    # Find most similar items\n",
    "    similarities = item_similarity[last_item]\n",
    "    most_similar = np.argsort(similarities)[-k_candidates-1:-1][::-1]  # Exclude self\n",
    "    \n",
    "    # Filter out seen items\n",
    "    seen = set(history)\n",
    "    recommendations = [int(item) for item in most_similar if item not in seen]\n",
    "    \n",
    "    return recommendations[:k_candidates]\n",
    "\n",
    "# Build interaction matrix\n",
    "print(\"Building user-item interaction matrix...\")\n",
    "interaction_matrix, user_histories = build_user_item_matrix(data['train'])\n",
    "print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "print(f\"Matrix density: {interaction_matrix.nnz / (interaction_matrix.shape[0] * interaction_matrix.shape[1]):.6f}\")\n",
    "\n",
    "# Create simple item similarity matrix (for demonstration)\n",
    "print(\"Computing item similarities...\")\n",
    "n_items = interaction_matrix.shape[1]\n",
    "\n",
    "# For large datasets, you'd compute this more efficiently\n",
    "if n_items <= 1000:  # Only for small datasets\n",
    "    # Compute item-item cosine similarity\n",
    "    item_profiles = interaction_matrix.T.toarray()  # items x users\n",
    "    norms = np.linalg.norm(item_profiles, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1  # Avoid division by zero\n",
    "    item_profiles_norm = item_profiles / norms\n",
    "    item_similarity = item_profiles_norm @ item_profiles_norm.T\n",
    "    print(f\"Computed item similarity matrix: {item_similarity.shape}\")\n",
    "else:\n",
    "    # Use content-based similarity as approximation for large datasets\n",
    "    if content_recommender.item_embeddings is not None:\n",
    "        item_similarity = content_recommender.item_embeddings @ content_recommender.item_embeddings.T\n",
    "        print(f\"Using content-based similarity: {item_similarity.shape}\")\n",
    "    else:\n",
    "        item_similarity = np.eye(n_items)  # Identity matrix fallback\n",
    "        print(\"Using identity matrix as fallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEnsembleRecommender:\n",
    "    \"\"\"Hybrid ensemble combining multiple recommendation approaches.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 popular_articles: pd.DataFrame,\n",
    "                 content_recommender: ContentBasedRecommender,\n",
    "                 user_histories: List[List[int]],\n",
    "                 item_similarity: np.ndarray):\n",
    "        self.popular_articles = popular_articles\n",
    "        self.content_recommender = content_recommender\n",
    "        self.user_histories = user_histories\n",
    "        self.item_similarity = item_similarity\n",
    "        \n",
    "        # Cache popular items for cold start\n",
    "        self.popular_items = popular_articles['click_article_id'].head(200).tolist()\n",
    "    \n",
    "    def classify_user(self, user_id: int, min_interactions: int = 5) -> str:\n",
    "        \"\"\"Classify user as cold or warm based on interaction history.\"\"\"\n",
    "        if user_id >= len(self.user_histories):\n",
    "            return \"cold\"\n",
    "        \n",
    "        n_interactions = len(self.user_histories[user_id])\n",
    "        return \"warm\" if n_interactions >= min_interactions else \"cold\"\n",
    "    \n",
    "    def recommend_cold_user(self, k: int = 10) -> List[int]:\n",
    "        \"\"\"Recommend popular items for cold users.\"\"\"\n",
    "        return self.popular_items[:k]\n",
    "    \n",
    "    def recommend_warm_user(self, user_id: int, k: int = 10) -> List[int]:\n",
    "        \"\"\"Generate ensemble recommendations for warm users.\"\"\"\n",
    "        user_history = self.user_histories[user_id] if user_id < len(self.user_histories) else []\n",
    "        \n",
    "        # Get candidates from different approaches\n",
    "        candidates = []\n",
    "        \n",
    "        # 1. Collaborative filtering (30% weight)\n",
    "        cf_candidates = collaborative_filtering_last_item(\n",
    "            self.user_histories, self.item_similarity, user_id, k_candidates=50\n",
    "        )\n",
    "        candidates.extend([(item, 0.3, 'cf') for item in cf_candidates[:20]])\n",
    "        \n",
    "        # 2. Content-based (40% weight)\n",
    "        content_candidates = self.content_recommender.recommend(user_history, k_candidates=50)\n",
    "        candidates.extend([(item, 0.4, 'content') for item in content_candidates[:20]])\n",
    "        \n",
    "        # 3. Popularity fallback (30% weight)\n",
    "        pop_candidates = [item for item in self.popular_items if item not in set(user_history)]\n",
    "        candidates.extend([(item, 0.3, 'popularity') for item in pop_candidates[:20]])\n",
    "        \n",
    "        # Aggregate scores and deduplicate\n",
    "        item_scores = {}\n",
    "        for item, weight, source in candidates:\n",
    "            if item not in item_scores:\n",
    "                item_scores[item] = 0\n",
    "            item_scores[item] += weight\n",
    "        \n",
    "        # Sort by aggregated score and return top-k\n",
    "        sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [item for item, score in sorted_items[:k]]\n",
    "    \n",
    "    def recommend(self, user_id: int, k: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"Generate recommendations with metadata.\"\"\"\n",
    "        user_type = self.classify_user(user_id)\n",
    "        \n",
    "        if user_type == \"cold\":\n",
    "            recommendations = self.recommend_cold_user(k)\n",
    "            algorithm_used = \"popularity\"\n",
    "        else:\n",
    "            recommendations = self.recommend_warm_user(user_id, k)\n",
    "            algorithm_used = \"ensemble\"\n",
    "        \n",
    "        return {\n",
    "            \"user_id\": user_id,\n",
    "            \"user_type\": user_type,\n",
    "            \"algorithm_used\": algorithm_used,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"n_recommendations\": len(recommendations)\n",
    "        }\n",
    "\n",
    "# Initialize ensemble recommender\n",
    "print(\"Initializing hybrid ensemble recommender...\")\n",
    "ensemble_recommender = HybridEnsembleRecommender(\n",
    "    popular_articles, content_recommender, user_histories, item_similarity\n",
    ")\n",
    "\n",
    "# Test recommendations\n",
    "print(\"\\nTesting recommendations:\")\n",
    "for test_user in [0, 10, 999]:  # Test different user types\n",
    "    result = ensemble_recommender.recommend(test_user, k=5)\n",
    "    print(f\"User {test_user} ({result['user_type']}): {result['recommendations'][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommender(recommender, validation_df: pd.DataFrame, k_list: List[int] = [5, 10, 20]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate recommender performance on validation set.\"\"\"\n",
    "    # Build ground truth\n",
    "    ground_truth = {}\n",
    "    for _, row in validation_df.iterrows():\n",
    "        user_id = int(row['user_id'])\n",
    "        item_id = int(row['click_article_id'])\n",
    "        if user_id not in ground_truth:\n",
    "            ground_truth[user_id] = set()\n",
    "        ground_truth[user_id].add(item_id)\n",
    "    \n",
    "    # Evaluate each user\n",
    "    metrics = {f'hit_rate@{k}': 0.0 for k in k_list}\n",
    "    metrics.update({f'precision@{k}': 0.0 for k in k_list})\n",
    "    metrics['mean_rank'] = 0.0\n",
    "    \n",
    "    total_users = 0\n",
    "    total_items = 0\n",
    "    \n",
    "    for user_id, true_items in ground_truth.items():\n",
    "        if not true_items:\n",
    "            continue\n",
    "        \n",
    "        # Get recommendations\n",
    "        result = recommender.recommend(user_id, k=max(k_list))\n",
    "        recommendations = result['recommendations']\n",
    "        \n",
    "        if not recommendations:\n",
    "            continue\n",
    "        \n",
    "        total_users += 1\n",
    "        \n",
    "        # Compute metrics for each k\n",
    "        for k in k_list:\n",
    "            top_k_recs = set(recommendations[:k])\n",
    "            hits = len(top_k_recs & true_items)\n",
    "            \n",
    "            # Hit rate (recall)\n",
    "            metrics[f'hit_rate@{k}'] += hits / len(true_items)\n",
    "            \n",
    "            # Precision\n",
    "            metrics[f'precision@{k}'] += hits / k if k > 0 else 0\n",
    "        \n",
    "        # Mean rank\n",
    "        for item in true_items:\n",
    "            if item in recommendations:\n",
    "                rank = recommendations.index(item) + 1\n",
    "            else:\n",
    "                rank = len(recommendations) + 1\n",
    "            metrics['mean_rank'] += rank\n",
    "            total_items += 1\n",
    "    \n",
    "    # Average metrics\n",
    "    if total_users > 0:\n",
    "        for key in metrics:\n",
    "            if 'mean_rank' in key:\n",
    "                metrics[key] = metrics[key] / total_items if total_items > 0 else float('inf')\n",
    "            else:\n",
    "                metrics[key] = metrics[key] / total_users\n",
    "    \n",
    "    metrics['total_users'] = total_users\n",
    "    metrics['total_items'] = total_items\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate ensemble recommender\n",
    "print(\"Evaluating hybrid ensemble recommender...\")\n",
    "ensemble_metrics = evaluate_recommender(ensemble_recommender, data['valid'])\n",
    "\n",
    "print(\"\\nEnsemble Recommender Results:\")\n",
    "for metric, value in ensemble_metrics.items():\n",
    "    if isinstance(value, float) and 'total' not in metric:\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    elif 'total' in metric:\n",
    "        print(f\"  {metric}: {int(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_temporal_popularity(val_df: pd.DataFrame, popular_articles: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Find optimal hyperparameters for temporal popularity model.\"\"\"\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "    \n",
    "    # Grid search over hyperparameters (limited for demo)\n",
    "    param_grid = {\n",
    "        'half_life_days': [1, 3, 7],\n",
    "        'beta': [0.0, 3.0, 10.0],\n",
    "        'alpha': [0.2, 0.5, 1.0],\n",
    "        'fresh_days': [1, 3, 7]\n",
    "    }\n",
    "    \n",
    "    print(\"Optimizing temporal popularity hyperparameters...\")\n",
    "    n_combinations = len(list(product(*param_grid.values())))\n",
    "    print(f\"Testing {n_combinations} parameter combinations\")\n",
    "    \n",
    "    for i, (half_life, beta, alpha, fresh_days) in enumerate(product(*param_grid.values())):\n",
    "        if i % 10 == 0:  # Progress update\n",
    "            print(f\"Progress: {i+1}/{n_combinations}\")\n",
    "        \n",
    "        try:\n",
    "            metrics = evaluate_temporal_popularity(\n",
    "                val_df, popular_articles, half_life, beta, alpha, fresh_days\n",
    "            )\n",
    "            \n",
    "            # Use recall@200 as optimization target\n",
    "            score = metrics['recall@200']\n",
    "            \n",
    "            result = {\n",
    "                'half_life_days': half_life,\n",
    "                'beta': beta,\n",
    "                'alpha': alpha,\n",
    "                'fresh_days': fresh_days,\n",
    "                'score': score,\n",
    "                **metrics\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = result.copy()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with params {half_life}, {beta}, {alpha}, {fresh_days}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score,\n",
    "        'all_results': results\n",
    "    }\n",
    "\n",
    "# Run hyperparameter optimization (simplified for demo)\n",
    "print(\"\\nRunning hyperparameter optimization...\")\n",
    "optimization_results = optimize_temporal_popularity(data['valid'], popular_articles)\n",
    "\n",
    "print(f\"\\nBest temporal popularity parameters:\")\n",
    "best_params = optimization_results['best_params']\n",
    "for key, value in best_params.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_user_distribution(user_histories: List[List[int]], ensemble_recommender) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze user distribution and recommendation patterns.\"\"\"\n",
    "    user_types = {'cold': 0, 'warm': 0}\n",
    "    interaction_counts = []\n",
    "    \n",
    "    for user_id in range(len(user_histories)):\n",
    "        n_interactions = len(user_histories[user_id])\n",
    "        interaction_counts.append(n_interactions)\n",
    "        \n",
    "        user_type = ensemble_recommender.classify_user(user_id)\n",
    "        user_types[user_type] += 1\n",
    "    \n",
    "    interaction_counts = np.array(interaction_counts)\n",
    "    \n",
    "    return {\n",
    "        'total_users': len(user_histories),\n",
    "        'cold_users': user_types['cold'],\n",
    "        'warm_users': user_types['warm'],\n",
    "        'cold_user_percentage': user_types['cold'] / len(user_histories) * 100,\n",
    "        'mean_interactions': interaction_counts.mean(),\n",
    "        'median_interactions': np.median(interaction_counts),\n",
    "        'max_interactions': interaction_counts.max(),\n",
    "        'users_with_no_interactions': (interaction_counts == 0).sum()\n",
    "    }\n",
    "\n",
    "def analyze_recommendation_diversity(ensemble_recommender, sample_users: List[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze diversity of recommendations across users.\"\"\"\n",
    "    if sample_users is None:\n",
    "        sample_users = list(range(min(100, len(user_histories))))\n",
    "    \n",
    "    all_recommendations = set()\n",
    "    algorithm_usage = {'popularity': 0, 'ensemble': 0}\n",
    "    \n",
    "    for user_id in sample_users:\n",
    "        result = ensemble_recommender.recommend(user_id, k=10)\n",
    "        all_recommendations.update(result['recommendations'])\n",
    "        algorithm_usage[result['algorithm_used']] += 1\n",
    "    \n",
    "    return {\n",
    "        'total_unique_items': len(all_recommendations),\n",
    "        'users_analyzed': len(sample_users),\n",
    "        'popularity_algorithm_usage': algorithm_usage['popularity'],\n",
    "        'ensemble_algorithm_usage': algorithm_usage['ensemble'],\n",
    "        'diversity_score': len(all_recommendations) / (len(sample_users) * 10)  # Normalized diversity\n",
    "    }\n",
    "\n",
    "# Analyze user distribution\n",
    "print(\"Analyzing user distribution...\")\n",
    "user_analysis = analyze_user_distribution(user_histories, ensemble_recommender)\n",
    "\n",
    "print(\"\\nUser Distribution Analysis:\")\n",
    "for key, value in user_analysis.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Analyze recommendation diversity\n",
    "print(\"\\nAnalyzing recommendation diversity...\")\n",
    "diversity_analysis = analyze_recommendation_diversity(ensemble_recommender)\n",
    "\n",
    "print(\"\\nRecommendation Diversity Analysis:\")\n",
    "for key, value in diversity_analysis.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Summary\n",
    "\n",
    "This notebook demonstrates a comprehensive hybrid ensemble recommendation system that combines:\n",
    "\n",
    "### ðŸŽ¯ **Key Components Implemented:**\n",
    "1. **Temporal Popularity Model**: Time-decay and freshness boost for trending content\n",
    "2. **Content-Based Filtering**: Embedding-based similarity using FAISS\n",
    "3. **Collaborative Filtering**: Item-item similarity for personalization\n",
    "4. **Ensemble Strategy**: Weighted combination of multiple approaches\n",
    "5. **Cold Start Handling**: Popularity fallback for new users\n",
    "\n",
    "### ðŸ“Š **Performance Insights:**\n",
    "- **Ensemble Approach**: Combines strengths of different algorithms\n",
    "- **User Segmentation**: Adapts strategy based on user interaction history\n",
    "- **Temporal Dynamics**: Captures trending and fresh content preferences\n",
    "- **Scalability**: Efficient implementation using sparse matrices and FAISS\n",
    "\n",
    "### ðŸš€ **Next Steps for Production:**\n",
    "1. **Real-time Updates**: Implement online learning for dynamic recommendations\n",
    "2. **A/B Testing**: Framework for comparing algorithm variants\n",
    "3. **Feature Engineering**: Add more contextual features (time, location, device)\n",
    "4. **Deep Learning**: Integrate neural collaborative filtering or transformers\n",
    "5. **Business Logic**: Add content filtering, diversity constraints, and business rules\n",
    "\n",
    "### ðŸ’¡ **Key Learnings:**\n",
    "- **No Single Algorithm**: Ensemble approaches consistently outperform individual methods\n",
    "- **Context Matters**: Temporal and user context significantly impact relevance\n",
    "- **Cold Start**: Popular content provides good baseline for new users\n",
    "- **Evaluation**: Multiple metrics needed to assess different aspects of performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}